{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c262b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b35f7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(SRCNN, self).__init__()\n",
    "        # Larger kernels for better receptive field\n",
    "        self.layer1 = nn.Conv2d(num_channels, 128, kernel_size=11, padding=5, stride=1)  # Big kernel for initial feature extraction\n",
    "        self.layer2 = nn.Conv2d(128, 128, kernel_size=9, padding=4, stride=1)           # Still big\n",
    "        self.layer3 = nn.Conv2d(128, 64, kernel_size=7, padding=3, stride=1)            # Slightly smaller\n",
    "        self.layer4 = nn.Conv2d(64, 64, kernel_size=5, padding=2, stride=1)\n",
    "        self.layer5 = nn.Conv2d(64, num_channels, kernel_size=5, padding=2, stride=1)   # Output layer\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.layer5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e2e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_images = sorted(os.listdir(lr_dir))\n",
    "        self.hr_images = sorted(os.listdir(hr_dir))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lr_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lr_path = os.path.join(self.lr_dir, self.lr_images[idx])\n",
    "        hr_path = os.path.join(self.hr_dir, self.hr_images[idx])\n",
    "        \n",
    "        lr_img = Image.open(lr_path).convert(\"YCbCr\").split()[0]\n",
    "        hr_img = Image.open(hr_path).convert(\"YCbCr\").split()[0]\n",
    "        \n",
    "        if self.transform:\n",
    "            lr_img = self.transform(lr_img)\n",
    "            hr_img = self.transform(hr_img)\n",
    "        \n",
    "        return lr_img, hr_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cf2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(pred, target):\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100.0\n",
    "    return 20 * math.log10(1.0 / math.sqrt(mse.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6215fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    This runs AFTER each training epoch\n",
    "    Calculates validation loss and PSNR\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_psnr = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient calculation\n",
    "        for lr, hr in val_loader:\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            output = model(lr)\n",
    "            loss = criterion(output, hr)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_psnr += calculate_psnr(output, hr)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_psnr = val_psnr / len(val_loader)\n",
    "    \n",
    "    return avg_val_loss, avg_val_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4          # Try: 8, 16, 32\n",
    "LEARNING_RATE = 1e-4     # Try: 1e-3, 1e-4, 5e-5\n",
    "NUM_EPOCHS = 10         # Try: 50, 100, 150\n",
    "WEIGHT_DECAY = 1e-5      # L2 regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c6195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 5. Data Preparation\n",
    "# -----------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e983701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_dataset = SRDataset(\"/home/akanksh/Akshu/ML/project/dataset/train/low_res\", \"/home/akanksh/Akshu/ML/project/dataset/train/high_res\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_dataset = SRDataset(\"/home/akanksh/Akshu/ML/project/dataset/val/low_res\", \"/home/akanksh/Akshu/ML/project/dataset/val/high_res\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f355117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "Model parameters: 1848385\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SRCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cead375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STARTED\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 3.80 GiB of which 212.44 MiB is free. Including non-PyTorch memory, this process has 3.58 GiB memory in use. Of the allocated memory 3.25 GiB is allocated by PyTorch, and 258.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m output = model(lr)\n\u001b[32m     20\u001b[39m loss = criterion(output, hr)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m     24\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Akshu/ML/project/.venv/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Akshu/ML/project/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Akshu/ML/project/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 3.80 GiB of which 212.44 MiB is free. Including non-PyTorch memory, this process has 3.58 GiB memory in use. Of the allocated memory 3.25 GiB is allocated by PyTorch, and 258.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 7. TRAINING LOOP\n",
    "# -----------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING STARTED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_psnr = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ========== TRAINING PHASE ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_psnr = 0.0\n",
    "    \n",
    "    for lr, hr in train_loader:\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(lr)\n",
    "        loss = criterion(output, hr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_psnr += calculate_psnr(output, hr)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_psnr = train_psnr / len(train_loader)\n",
    "    \n",
    "    # ========== VALIDATION PHASE (SIMULTANEOUS) ==========\n",
    "    avg_val_loss, avg_val_psnr = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "    print(f\"  Train -> Loss: {avg_train_loss:.6f} | PSNR: {avg_train_psnr:.2f} dB\")\n",
    "    print(f\"  Val   -> Loss: {avg_val_loss:.6f} | PSNR: {avg_val_psnr:.2f} dB\")\n",
    "    \n",
    "    # Save best model based on validation PSNR\n",
    "    if avg_val_psnr > best_val_psnr:\n",
    "        best_val_psnr = avg_val_psnr\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"  ✅ Best model saved! (Val PSNR: {avg_val_psnr:.2f} dB)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_class, model_path, input_dir, output_dir, device):\n",
    "    \"\"\"\n",
    "    Run super-resolution inference on all images in input_dir\n",
    "    and save results to output_dir.\n",
    "    \"\"\"\n",
    "    # 1️⃣ Load the model\n",
    "    model = model_class().to(device)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define preprocessing transform\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # 2️⃣ Process images\n",
    "    image_files = sorted([\n",
    "        f for f in os.listdir(input_dir)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ])\n",
    "\n",
    "    print(f\"Processing {len(image_files)} images...\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_name in image_files:\n",
    "            lr_img = Image.open(os.path.join(input_dir, img_name)).convert(\"YCbCr\")\n",
    "            y, cb, cr = lr_img.split()\n",
    "\n",
    "            y_tensor = transform(y).unsqueeze(0).to(device)\n",
    "            sr_y = model(y_tensor).clamp(0.0, 1.0)\n",
    "            sr_y_img = transforms.ToPILImage()(sr_y.squeeze(0).cpu())\n",
    "\n",
    "            cb = cb.resize(sr_y_img.size, Image.BICUBIC)\n",
    "            cr = cr.resize(sr_y_img.size, Image.BICUBIC)\n",
    "            sr_img = Image.merge(\"YCbCr\", [sr_y_img, cb, cr]).convert(\"RGB\")\n",
    "\n",
    "            sr_img.save(os.path.join(output_dir, f\"SR_{img_name}\"))\n",
    "            print(f\"✅ Saved: SR_{img_name}\")\n",
    "#\n",
    "    # print(f\"\\n✅ All {len(image_files)} images processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80b848b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SRCNN:\n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([64, 1, 9, 9]) from checkpoint, the shape in current model is torch.Size([1024, 1, 9, 9]).\n\tsize mismatch for layer1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer2.weight: copying a param with shape torch.Size([64, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 5, 5]).\n\tsize mismatch for layer2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([32, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 512, 5, 5]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer5.weight: copying a param with shape torch.Size([1, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([1, 128, 5, 5]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSRCNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# your SRCNN class\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/akanksh/Akshu/ML/project/best_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# path to saved model\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/akanksh/Akshu/ML/project/dataset/raw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# LR images\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/akanksh/Akshu/ML/project/dataset/out\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# SR outputs\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36minference\u001b[39m\u001b[34m(model_class, model_path, input_dir, output_dir, device)\u001b[39m\n\u001b[32m      7\u001b[39m model = model_class().to(device)\n\u001b[32m      8\u001b[39m state_dict = torch.load(model_path, map_location=device)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m model.eval()\n\u001b[32m     11\u001b[39m os.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Akshu/ML/project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for SRCNN:\n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([64, 1, 9, 9]) from checkpoint, the shape in current model is torch.Size([1024, 1, 9, 9]).\n\tsize mismatch for layer1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer2.weight: copying a param with shape torch.Size([64, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 5, 5]).\n\tsize mismatch for layer2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([32, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 512, 5, 5]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer5.weight: copying a param with shape torch.Size([1, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([1, 128, 5, 5])."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inference(\n",
    "    model_class=SRCNN,                     # your SRCNN class\n",
    "    model_path=\"/home/akanksh/Akshu/ML/project/best_model.pth\",           # path to saved model\n",
    "    input_dir=\"/home/akanksh/Akshu/ML/project/dataset/raw\",      # LR images\n",
    "    output_dir=\"/home/akanksh/Akshu/ML/project/dataset/out\",  # SR outputs\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c5cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
