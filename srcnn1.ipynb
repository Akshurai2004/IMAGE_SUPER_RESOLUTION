{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c262b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(SRCNN, self).__init__()\n",
    "        # Original: 3 layers\n",
    "        # Improved: 5 layers for better feature extraction\n",
    "        self.layer1 = nn.Conv2d(num_channels, 64, kernel_size=512, padding=4,stride=1)\n",
    "        self.layer2 = nn.Conv2d(64, 64, kernel_size=256, padding=2,stride=1)\n",
    "        self.layer3 = nn.Conv2d(64, 32, kernel_size=128, padding=2,stride=1)\n",
    "        self.layer4 = nn.Conv2d(32, 32, kernel_size=256, padding=1,stride=1)\n",
    "        self.layer5 = nn.Conv2d(32, num_channels, kernel_size=512, padding=2,stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.layer5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57e2e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_images = sorted(os.listdir(lr_dir))\n",
    "        self.hr_images = sorted(os.listdir(hr_dir))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.lr_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lr_path = os.path.join(self.lr_dir, self.lr_images[idx])\n",
    "        hr_path = os.path.join(self.hr_dir, self.hr_images[idx])\n",
    "        \n",
    "        lr_img = Image.open(lr_path).convert(\"YCbCr\").split()[0]\n",
    "        hr_img = Image.open(hr_path).convert(\"YCbCr\").split()[0]\n",
    "        \n",
    "        if self.transform:\n",
    "            lr_img = self.transform(lr_img)\n",
    "            hr_img = self.transform(hr_img)\n",
    "        \n",
    "        return lr_img, hr_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5cf2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(pred, target):\n",
    "    mse = torch.mean((pred - target) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100.0\n",
    "    return 20 * math.log10(1.0 / math.sqrt(mse.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6215fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    This runs AFTER each training epoch\n",
    "    Calculates validation loss and PSNR\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_psnr = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient calculation\n",
    "        for lr, hr in val_loader:\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            output = model(lr)\n",
    "            loss = criterion(output, hr)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_psnr += calculate_psnr(output, hr)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_psnr = val_psnr / len(val_loader)\n",
    "    \n",
    "    return avg_val_loss, avg_val_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1edc6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16          # Try: 8, 16, 32\n",
    "LEARNING_RATE = 1e-4     # Try: 1e-3, 1e-4, 5e-5\n",
    "NUM_EPOCHS = 10         # Try: 50, 100, 150\n",
    "WEIGHT_DECAY = 1e-5      # L2 regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5c6195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 5. Data Preparation\n",
    "# -----------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e983701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_dataset = SRDataset(\"/home/akanksh/Akshu/ML/project/dataset/train/low_res\", \"/home/akanksh/Akshu/ML/project/dataset/train/high_res\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_dataset = SRDataset(\"/home/akanksh/Akshu/ML/project/dataset/val/low_res\", \"/home/akanksh/Akshu/ML/project/dataset/val/high_res\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f355117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "Model parameters: 168993\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SRCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9cead375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STARTED\n",
      "============================================================\n",
      "Epoch [1/10]\n",
      "  Train -> Loss: 0.001350 | PSNR: 28.77 dB\n",
      "  Val   -> Loss: 0.001333 | PSNR: 28.77 dB\n",
      "  ✅ Best model saved! (Val PSNR: 28.77 dB)\n",
      "\n",
      "Epoch [2/10]\n",
      "  Train -> Loss: 0.001319 | PSNR: 28.85 dB\n",
      "  Val   -> Loss: 0.001325 | PSNR: 28.80 dB\n",
      "  ✅ Best model saved! (Val PSNR: 28.80 dB)\n",
      "\n",
      "Epoch [3/10]\n",
      "  Train -> Loss: 0.001281 | PSNR: 29.01 dB\n",
      "  Val   -> Loss: 0.001256 | PSNR: 29.03 dB\n",
      "  ✅ Best model saved! (Val PSNR: 29.03 dB)\n",
      "\n",
      "Epoch [4/10]\n",
      "  Train -> Loss: 0.001229 | PSNR: 29.17 dB\n",
      "  Val   -> Loss: 0.001225 | PSNR: 29.14 dB\n",
      "  ✅ Best model saved! (Val PSNR: 29.14 dB)\n",
      "\n",
      "Epoch [5/10]\n",
      "  Train -> Loss: 0.001206 | PSNR: 29.24 dB\n",
      "  Val   -> Loss: 0.001203 | PSNR: 29.22 dB\n",
      "  ✅ Best model saved! (Val PSNR: 29.22 dB)\n",
      "\n",
      "Epoch [6/10]\n",
      "  Train -> Loss: 0.001227 | PSNR: 29.21 dB\n",
      "  Val   -> Loss: 0.001368 | PSNR: 28.66 dB\n",
      "\n",
      "Epoch [7/10]\n",
      "  Train -> Loss: 0.001179 | PSNR: 29.37 dB\n",
      "  Val   -> Loss: 0.001160 | PSNR: 29.38 dB\n",
      "  ✅ Best model saved! (Val PSNR: 29.38 dB)\n",
      "\n",
      "Epoch [8/10]\n",
      "  Train -> Loss: 0.001140 | PSNR: 29.53 dB\n",
      "  Val   -> Loss: 0.001141 | PSNR: 29.45 dB\n",
      "  ✅ Best model saved! (Val PSNR: 29.45 dB)\n",
      "\n",
      "Epoch [9/10]\n",
      "  Train -> Loss: 0.001124 | PSNR: 29.55 dB\n",
      "  Val   -> Loss: 0.001126 | PSNR: 29.51 dB\n",
      "  ✅ Best model saved! (Val PSNR: 29.51 dB)\n",
      "\n",
      "Epoch [10/10]\n",
      "  Train -> Loss: 0.001105 | PSNR: 29.63 dB\n",
      "  Val   -> Loss: 0.001112 | PSNR: 29.56 dB\n",
      "  ✅ Best model saved! (Val PSNR: 29.56 dB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. TRAINING LOOP\n",
    "# -----------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING STARTED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_psnr = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ========== TRAINING PHASE ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_psnr = 0.0\n",
    "    \n",
    "    for lr, hr in train_loader:\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(lr)\n",
    "        loss = criterion(output, hr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_psnr += calculate_psnr(output, hr)\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_psnr = train_psnr / len(train_loader)\n",
    "    \n",
    "    # ========== VALIDATION PHASE (SIMULTANEOUS) ==========\n",
    "    avg_val_loss, avg_val_psnr = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "    print(f\"  Train -> Loss: {avg_train_loss:.6f} | PSNR: {avg_train_psnr:.2f} dB\")\n",
    "    print(f\"  Val   -> Loss: {avg_val_loss:.6f} | PSNR: {avg_val_psnr:.2f} dB\")\n",
    "    \n",
    "    # Save best model based on validation PSNR\n",
    "    if avg_val_psnr > best_val_psnr:\n",
    "        best_val_psnr = avg_val_psnr\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"  ✅ Best model saved! (Val PSNR: {avg_val_psnr:.2f} dB)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_class, model_path, input_dir, output_dir, device):\n",
    "    \"\"\"\n",
    "    Run super-resolution inference on all images in input_dir\n",
    "    and save results to output_dir.\n",
    "    \"\"\"\n",
    "    # 1️⃣ Load the model\n",
    "    model = model_class().to(device)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define preprocessing transform\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # 2️⃣ Process images\n",
    "    image_files = sorted([\n",
    "        f for f in os.listdir(input_dir)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ])\n",
    "\n",
    "    print(f\"Processing {len(image_files)} images...\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_name in image_files:\n",
    "            lr_img = Image.open(os.path.join(input_dir, img_name)).convert(\"YCbCr\")\n",
    "            y, cb, cr = lr_img.split()\n",
    "\n",
    "            y_tensor = transform(y).unsqueeze(0).to(device)\n",
    "            sr_y = model(y_tensor).clamp(0.0, 1.0)\n",
    "            sr_y_img = transforms.ToPILImage()(sr_y.squeeze(0).cpu())\n",
    "\n",
    "            cb = cb.resize(sr_y_img.size, Image.BICUBIC)\n",
    "            cr = cr.resize(sr_y_img.size, Image.BICUBIC)\n",
    "            sr_img = Image.merge(\"YCbCr\", [sr_y_img, cb, cr]).convert(\"RGB\")\n",
    "\n",
    "            sr_img.save(os.path.join(output_dir, f\"SR_{img_name}\"))\n",
    "            print(f\"✅ Saved: SR_{img_name}\")\n",
    "#\n",
    "    # print(f\"\\n✅ All {len(image_files)} images processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80b848b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SRCNN:\n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([64, 1, 9, 9]) from checkpoint, the shape in current model is torch.Size([1024, 1, 9, 9]).\n\tsize mismatch for layer1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer2.weight: copying a param with shape torch.Size([64, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 5, 5]).\n\tsize mismatch for layer2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([32, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 512, 5, 5]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer5.weight: copying a param with shape torch.Size([1, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([1, 128, 5, 5]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSRCNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# your SRCNN class\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/akanksh/Akshu/ML/project/best_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# path to saved model\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/akanksh/Akshu/ML/project/dataset/raw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# LR images\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/akanksh/Akshu/ML/project/dataset/out\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# SR outputs\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36minference\u001b[39m\u001b[34m(model_class, model_path, input_dir, output_dir, device)\u001b[39m\n\u001b[32m      7\u001b[39m model = model_class().to(device)\n\u001b[32m      8\u001b[39m state_dict = torch.load(model_path, map_location=device)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m model.eval()\n\u001b[32m     11\u001b[39m os.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Akshu/ML/project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for SRCNN:\n\tsize mismatch for layer1.weight: copying a param with shape torch.Size([64, 1, 9, 9]) from checkpoint, the shape in current model is torch.Size([1024, 1, 9, 9]).\n\tsize mismatch for layer1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer2.weight: copying a param with shape torch.Size([64, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([1024, 1024, 5, 5]).\n\tsize mismatch for layer2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for layer3.weight: copying a param with shape torch.Size([32, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 512, 5, 5]).\n\tsize mismatch for layer3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for layer4.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer4.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer5.weight: copying a param with shape torch.Size([1, 32, 5, 5]) from checkpoint, the shape in current model is torch.Size([1, 128, 5, 5])."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inference(\n",
    "    model_class=SRCNN,                     # your SRCNN class\n",
    "    model_path=\"/home/akanksh/Akshu/ML/project/best_model.pth\",           # path to saved model\n",
    "    input_dir=\"/home/akanksh/Akshu/ML/project/dataset/raw\",      # LR images\n",
    "    output_dir=\"/home/akanksh/Akshu/ML/project/dataset/out\",  # SR outputs\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c5cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
